{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "batch_size =256\n",
    "num_workers = 4\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root='./Datasets',\n",
    "               train=True,download=True,transform=torchvision.transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='./Datasets',\n",
    "               train=False,download=True,transform=torchvision.transforms.ToTensor())\n",
    "train_iter = torch.utils.data.DataLoader(mnist_train,batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
    "test_iter = torch.utils.data.DataLoader(mnist_test,batch_size=batch_size,shuffle=True,num_workers=num_workers)\n",
    "print(mnist_train[0][0].size())  #公司自动下载的是对的，家里自动下载的是错的，所以网上自己找了资源放在相同的地方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32\n",
      "torch.Size([784, 10]) torch.Size([10])\n",
      "torch.Size([784, 10])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "w = torch.randn(num_inputs,num_outputs)\n",
    "#w = torch.FloatTensor(np.random.normal(0,1,(num_inputs,num_outputs)))\n",
    "b = torch.zeros(num_outputs)\n",
    "print(w.dtype,b.dtype)\n",
    "print(w.size(),b.size())\n",
    "w.requires_grad_(requires_grad=True)\n",
    "b.requires_grad_(requires_grad=True)\n",
    "params = [w,b]\n",
    "print(params[0].data.size())\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([1, 3])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(X.size())\n",
    "print(X.sum(0,keepdim=True).size())\n",
    "print(X.sum(1,keepdim=True).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2424, 0.2582, 0.1414, 0.1493, 0.2087],\n",
      "        [0.1984, 0.1437, 0.1494, 0.2555, 0.2530]]) tensor([1., 1.])\n",
      "tensor(2.0000)\n"
     ]
    }
   ],
   "source": [
    "def softmax(X):\n",
    "    X_exp = X.exp()  #X还要用，不能在原地计算  原地计算是exp_() \n",
    "    #=赋值在函数结束后不会改变变量值，exp_会彻底改变\n",
    "    partition = X_exp.sum(dim=1,keepdim=True)\n",
    "    #print(X_exp.size(),partition.size())\n",
    "    return X_exp/partition   #广播\n",
    "X = torch.rand(2, 5)\n",
    "X_prob = softmax(X)\n",
    "print(X_prob, X_prob.sum(dim=1))\n",
    "print(X_prob.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(torch.mm(X.view(-1,num_inputs),w)+b)\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y = torch.LongTensor([0, 2])\n",
    "#print(y.size(),y.view(-1,1).size())\n",
    "y_hat.gather(1, y.view(-1, 1))\n",
    "#print((y_hat.argmax(dim=1)==y).float().mean().item())\n",
    "\n",
    "def cross_entropy(y_hat,y):\n",
    "    return -torch.log(y_hat.gather(1,y.view(-1,1)))\n",
    "\n",
    "def accuracy(y_hat,y):\n",
    "    return (y_hat.argmax(dim=1)==y).float().mean().item()\n",
    "\n",
    "def evaluate_accuracy(data_iter,net):\n",
    "    acc_sum,n=0.0,0\n",
    "    for X,y in data_iter:\n",
    "        acc_sum += (net(X).argmax(dim=1)==y).float().sum().item()\n",
    "        n +=y.shape[0]\n",
    "        print(acc_sum,n)\n",
    "        break\n",
    "    return acc_sum/n\n",
    "#print(evaluate_accuracy(train_iter,net))\n",
    "\n",
    "def sgd(params,lr,batch_size):\n",
    "    for param in params:\n",
    "        param.data -= lr *param.grad/batch_size  #tensor.data就是数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148.0 256\n",
      "epoch 1, loss 3.9116, train acc 0.431, test acc 0.578\n",
      "162.0 256\n",
      "epoch 2, loss 1.9144, train acc 0.621, test acc 0.633\n",
      "176.0 256\n",
      "epoch 3, loss 1.5771, train acc 0.672, test acc 0.688\n",
      "167.0 256\n",
      "epoch 4, loss 1.4020, train acc 0.699, test acc 0.652\n",
      "193.0 256\n",
      "epoch 5, loss 1.2852, train acc 0.718, test acc 0.754\n",
      "196.0 256\n",
      "epoch 6, loss 1.2039, train acc 0.732, test acc 0.766\n",
      "194.0 256\n",
      "epoch 7, loss 1.1418, train acc 0.741, test acc 0.758\n",
      "191.0 256\n",
      "epoch 8, loss 1.0908, train acc 0.749, test acc 0.746\n",
      "189.0 256\n",
      "epoch 9, loss 1.0504, train acc 0.755, test acc 0.738\n",
      "193.0 256\n",
      "epoch 10, loss 1.0147, train acc 0.761, test acc 0.754\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 0.1\n",
    "for epoch in range(1,epochs+1):\n",
    "    train_l_sum,train_acc_sum,n=0.0,0.0,0\n",
    "    for X,y in train_iter:\n",
    "        y_hat = net(X)\n",
    "        loss = cross_entropy(y_hat,y).sum()\n",
    "        loss.backward()\n",
    "        sgd([w,b],lr,batch_size)\n",
    "        w.grad.data.zero_()   #w.grad在backward之前为None,要先backword()\n",
    "        b.grad.data.zero_()\n",
    "        train_l_sum +=loss.item()\n",
    "        train_acc_sum +=(y_hat.argmax(dim=1)==y).sum().item()\n",
    "        n += y.shape[0]\n",
    "    test_acc = evaluate_accuracy(test_iter,net)\n",
    "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "         %(epoch,train_l_sum/n,train_acc_sum/n,test_acc))  \n",
    "    #下的数据库test居然不对，而且训练越训越糟，应该是数据库的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]\n",
    "def show_fashion_mnist(images,labels):\n",
    "    figs,axs = plt.subplots(1,len(images),figsize=(12,12))\n",
    "    for ax,img,lbl in zip(axs,images,labels):\n",
    "        ax.imshow(img.view(28,28))\n",
    "        ax.set_title(lbl)\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False) \n",
    "\n",
    "X,y = iter(train_iter).next()\n",
    "X,y = iter(train_iter).next()\n",
    "true_labels = get_fashion_mnist_labels(y)  #y.numpy()不是必须的\n",
    "pred_labels = get_fashion_mnist_labels(net(X).argmax(dim=1))\n",
    "titles = [true + '\\n' + pred for true, pred in zip(true_labels, pred_labels)]\n",
    "\n",
    "show_fashion_mnist(X[0:9],titles[0:9])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
